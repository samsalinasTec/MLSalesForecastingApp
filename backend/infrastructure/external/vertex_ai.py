"""
LEARNING NOTE: IntegraciÃ³n con Vertex AI para LLM
Separado para cambiar fÃ¡cilmente de proveedor (OpenAI, Anthropic, etc.)
"""

from google.cloud import aiplatform
from vertexai.generative_models import GenerativeModel, GenerationConfig
import vertexai
from typing import Dict, Any, List, Optional
import json
import logging
from backend.core.config import settings
from backend.core.exceptions import VertexAIException

logger = logging.getLogger(__name__)

class VertexAIClient:
    """
    Cliente para interactuar con Gemini en Vertex AI
    
    LEARNING NOTE: Facade Pattern - Simplifica una API compleja
    """
    
    def __init__(self):
        """
        Inicializa configuraciÃ³n pero NO la conexiÃ³n
        LAZY LOADING: La conexiÃ³n real se hace cuando se usa
        """
        self._initialized = False
        self._model: Optional[GenerativeModel] = None
        self._generation_config: Optional[GenerationConfig] = None
        logger.info("VertexAIClient creado (sin conectar aÃºn)")
    
    def _ensure_initialized(self):
        """
        Lazy initialization - solo conecta cuando se necesita
        
        LEARNING NOTE: Este patrÃ³n evita conexiones costosas en el arranque
        """
        if not self._initialized:
            logger.info("ðŸ”„ Inicializando conexiÃ³n con Vertex AI...")
            
            try:
                vertexai.init(
                    project=settings.gcp_project_id,
                    location=settings.vertex_ai_location
                )
                
                self._model = GenerativeModel(settings.model_name)
                
                # ConfiguraciÃ³n para respuestas consistentes
                self._generation_config = GenerationConfig(
                    temperature=0.3,  # Baja temperatura = respuestas mÃ¡s consistentes
                    max_output_tokens=500,
                    top_p=0.8,
                    top_k=40
                )
                
                self._initialized = True
                logger.info("âœ… Vertex AI conectado exitosamente")
                
            except Exception as e:
                logger.error(f"âŒ Error conectando con Vertex AI: {str(e)}")
                raise VertexAIException(f"Failed to initialize Vertex AI: {str(e)}")
    
    @property
    def model(self) -> GenerativeModel:
        """Acceso lazy al modelo"""
        self._ensure_initialized()
        return self._model
    
    @property
    def generation_config(self) -> GenerationConfig:
        """Acceso lazy a la configuraciÃ³n"""
        self._ensure_initialized()
        return self._generation_config
        
    async def analyze_prediction_change(
        self,
        product_id: str,
        original_value: float,
        smoothed_value: float,
        historical_values: List[float],
        change_percentage: float
    ) -> str:
        """
        Usa el LLM para explicar cambios en predicciones
        
        LEARNING NOTE: Prompt engineering estructurado
        """
        
        # Solo inicializa cuando realmente se use
        self._ensure_initialized()
        
        # LEARNING NOTE: System prompt claro y especÃ­fico
        prompt = f"""
        Eres un analista de ventas experto. Analiza el siguiente cambio en la predicciÃ³n:
        
        Producto: {product_id}
        PredicciÃ³n original del modelo: ${original_value:,.0f}
        PredicciÃ³n suavizada: ${smoothed_value:,.0f}
        Cambio porcentual: {change_percentage:.1f}%
        
        Contexto histÃ³rico (Ãºltimos 10 dÃ­as): {historical_values}
        
        Proporciona una explicaciÃ³n breve y profesional (mÃ¡ximo 3 lÃ­neas) sobre:
        1. Por quÃ© se aplicÃ³ el suavizado
        2. Si el ajuste es razonable dado el contexto
        3. Una recomendaciÃ³n de acciÃ³n
        
        Formato: JSON con keys "explicacion", "es_razonable", "recomendacion"
        """
        
        try:
            response = self.model.generate_content(
                prompt,
                generation_config=self.generation_config
            )
            
            # Intentar parsear como JSON
            try:
                result = json.loads(response.text)
                return result.get("explicacion", response.text)
            except json.JSONDecodeError:
                # Si no es JSON vÃ¡lido, retornar texto plano
                return response.text
                
        except Exception as e:
            logger.error(f"Error en Vertex AI: {str(e)}")
            # Fallback sin LLM
            return f"Cambio de {change_percentage:.1f}% detectado. Suavizado aplicado para mantener consistencia."
    
    async def get_market_insights(
        self,
        product_id: str,
        predictions: Dict[str, float]
    ) -> Dict[str, Any]:
        """
        Obtiene insights de mercado para las predicciones
        
        PREGUNTA: Â¿Quieres que el LLM sugiera acciones especÃ­ficas?
        Por ejemplo: ajustar inventario, promociones, etc.
        """
        
        # Solo inicializa cuando realmente se use
        self._ensure_initialized()
        
        prompt = f"""
        Analiza las siguientes predicciones de venta y proporciona insights:
        
        Predicciones por producto:
        {json.dumps(predictions, indent=2)}
        
        Proporciona:
        1. Tendencia general del mercado
        2. Productos con mejor desempeÃ±o esperado
        3. Alertas o riesgos identificados
        4. 2-3 acciones recomendadas
        
        Formato: JSON estructurado
        """
        
        try:
            response = self.model.generate_content(
                prompt,
                generation_config=self.generation_config
            )
            
            return json.loads(response.text)
            
        except Exception as e:
            logger.error(f"Error obteniendo insights: {str(e)}")
            return {
                "tendencia": "No disponible",
                "acciones": ["Revisar predicciones manualmente"]
            }